{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Station Information and Update**\n",
        "\n",
        "We handle each station individually because that is how the algorithms are described and it lets us simulate the parellelism of the true process by telling each station what it's local clock reads at instances A,B,n,B' at the start of each iteration. This is done later in \"set_local_times\".\n",
        "\n",
        "In addition to the local times, each station also has a global estimate of time instance n, an id number, it's bias and drift estimates(x), it's estimate covariance (P), the estimates after the cross link updates (psi), and it's nbhr station id's as well as how far away the nbhr station is (Dij = nbhr_dist[j])\n",
        "\n",
        "Then, each station has it's time update, it's GPS update(a.k.a incremental update), it's cross link update, and it's diffusion update as outlined in the time transfer paper. \n",
        "\n",
        "There is also a function second_cross_link which finds the T_g,B' which is shared in the second cross link measurements from the calculated values of psi, T_l,n, and D_ij\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Cross Link Covariance**\n",
        "\n",
        "We also deal with the cross link covariance here (sigma_j in Eqn 38) . The paper doesn't specify what values they used but when we assumed it was small (0.001) our filter was VERY smug(small estimate covariance), but when we increased it the filter seemed to perform better. It is highlighted below in the cross-link-update function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "FbVO4q3Z3yXv"
      },
      "outputs": [],
      "source": [
        "class Station:\n",
        "    def __init__(self,id):\n",
        "        self.local_time = None\n",
        "        self.global_est = 0 #\\hat{T}_g,n\n",
        "        self.id = id\n",
        "        self.x = None\n",
        "        self.P = None\n",
        "        self.psi_n = None\n",
        "        self.nbhrs = []\n",
        "        self.nbhr_dist = {}\n",
        "\n",
        "    def time_update(self, Q,delta_t = 60):\n",
        "        # time update steps\n",
        "        A = np.array([[1, delta_t], [0, 1]])\n",
        "        self.x = A @ self.x\n",
        "        self.P = A @ self.P @ A.T + Q\n",
        "\n",
        "    def incremental_update(self,R,z):\n",
        "    # Calculate pseudorange/range-rate residuals using Eqs. (23)-(25), add in speed of light\n",
        "        N = int(len(z)/2)\n",
        "\n",
        "        C = np.block([\n",
        "                [np.ones((N, 1)), np.zeros((N, 1))],\n",
        "                [np.zeros((N, 1)), np.ones((N, 1))]\n",
        "            ])\n",
        "\n",
        "        # Update the state and covariance estimate with Eqs. (26)-(28)\n",
        "        K_n = self.P @ C.T @ np.linalg.inv((C @ self.P @ C.T) + R(N)) # Kalman gain matrix; R defined earlier\n",
        "        x_hat_n = self.x + (K_n @ (z - C @ self.x))\n",
        "        P_hat_n = (np.eye(2,2) - K_n @ C) @ self.P\n",
        "\n",
        "        self.x = x_hat_n\n",
        "        self.P = P_hat_n\n",
        "\n",
        "\n",
        "\n",
        "    def crosslink_update(self,nbhr_time_ests,nbhr_cov_ests):\n",
        "        H = np.array([[1, 0]])\n",
        "        self.psi_n = np.copy(self.x)\n",
        "        P_hat_n = np.copy(self.P)\n",
        "        \n",
        "        for nbhr_id in self.nbhrs:\n",
        "            #Eqn 16\n",
        "            z_jB = c*(self.local_time[\"B\"]-nbhr_time_ests[nbhr_id] - self.nbhr_dist[nbhr_id])\n",
        "\n",
        "            #Eqn 34\n",
        "            z_jn = np.array([[z_jB + self.x[1][0]*(self.local_time[\"n\"] - self.local_time[\"B\"])]])\n",
        "            \n",
        "\n",
        "            #HERE IS THE CROSS LINK COVARIANCE!!\n",
        "            sigma_j = 0.01\n",
        "            R_j = np.array([[sigma_j + nbhr_cov_ests[nbhr_id][0][0]]])\n",
        "\n",
        "            K_ij_n = P_hat_n @ H.T @ np.linalg.inv((H @ P_hat_n @ H.T) + R_j)\n",
        "            self.psi_n = self.psi_n + (K_ij_n @ (z_jn - H @ self.psi_n))\n",
        "            P_hat_n = (np.eye(2,2) - K_ij_n @ H) @ P_hat_n\n",
        "        \n",
        "        self.P = P_hat_n  \n",
        "          \n",
        "    #This simulated T_g,B' to send to another station using our T_g,n\n",
        "    def second_cross_link(self,D_ij):\n",
        "        return self.local_time[\"n\"] - ((1/c)*self.psi_n[0][0]) + D_ij\n",
        "\n",
        "    def diffusion_update(self,B,nbhr_time_est):\n",
        "        #The nbhr_time_est are the T_g,B' from our neighbors\n",
        "        my_T_hat_g_n = self.local_time[\"n\"] - (1/c)*self.psi_n[0][0]\n",
        "\n",
        "        #This is our extrapolation of T_g,n from T_g,B\"\n",
        "        T_hat_g_n = {nbhr_id:nbhr_time_est[nbhr_id]+(1-(self.x[1][0]/c))*(self.local_time['n'] - self.local_time['Bp']) for nbhr_id in self.nbhrs}\n",
        "        \n",
        "        #Actual DIffusion step\n",
        "        self.global_est = sum([B[self.id][nbhr_id]*T_hat_g_n[nbhr_id] for nbhr_id in self.nbhrs]+[B[self.id][self.id]*my_T_hat_g_n])\n",
        "        self.x = np.array([[c*(self.local_time[\"n\"] - self.global_est)],self.x[1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Measurement Simulation**\n",
        "\n",
        "As opposed to simulating astrodynamics to generate measurements, we instead keep track of the true global time as well as the true bias and drift of each station. With this, we are able to simulate the psuedorange and psuedorange rate residual measurements by simply adding measurement noise, drawn from a zero mean distribution with our measurement covariance (R).\n",
        "\n",
        "This computation to find the measurements is done below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "_dt5REjz3yXv"
      },
      "outputs": [],
      "source": [
        "def measurement_noise(R,N_i):\n",
        "     return np.linalg.cholesky(R(N_i))@np.random.normal(np.array([[0 for _ in range(2*N_i)]]).T)\n",
        "\n",
        "def GPS_measurements(true_bias, true_drift,R,N_i = 2):\n",
        "\n",
        "     true_pr = np.array([[c*(true_bias[0])] for _ in range(N_i)])\n",
        "\n",
        "     true_prr = np.array([[c*(true_drift[0])] for _ in range(N_i)])\n",
        "\n",
        "     true_measurements = np.vstack((true_pr,true_prr))\n",
        "\n",
        "     return true_measurements+measurement_noise(R,N_i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Single Filter Iteration**\n",
        "\n",
        "This is where we simulate a single iteration of our filter. We have a function to setup the filter before the first iteration. This includes giving each station it's initial estimate as well as setting it's neighbors and neighbor distances. \n",
        "\n",
        "We also have the aforementioned functions to set the local times at each station. We assume that the algorithms take zero time and each cross link communication takes 3 seconds. We set the local times to match this. \n",
        "\n",
        "To run a single iteration of the filter we can set how many satellites are visible to each station (it defaults to 2 per station) and then we set the local times based off the true times and true biases and run each step. We return the measurements so we can plot them later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "Czj-vPjVZsoJ"
      },
      "outputs": [],
      "source": [
        "def filter_initialize(stations,adj_mat,x_initial,P_initial):\n",
        "  M = len(stations)\n",
        "  for i in range(M):\n",
        "    stations[i].x = x_initial[i]\n",
        "    stations[i].P = P_initial[i]\n",
        "\n",
        "    for j in range(i+1,M):\n",
        "        if adj_mat[i][j]!=0:\n",
        "          stations[i].nbhrs.append(j)\n",
        "          stations[j].nbhrs.append(i)\n",
        "          stations[i].nbhr_dist[j] = adj_mat[i][j]\n",
        "          stations[j].nbhr_dist[i] = adj_mat[i][j]\n",
        "\n",
        "def new_local_times(true_time,true_bias,true_drift):\n",
        "   T_A = true_time+true_bias\n",
        "   T_B = T_A+3+(true_drift*3)\n",
        "   T_n = T_B\n",
        "   T_Bp = T_n+3+(true_drift*3)\n",
        "\n",
        "   return {'A':T_A,'B':T_B,'n':T_n,'Bp':T_Bp}\n",
        "\n",
        "\n",
        "def diffusion_filter_iteration(stations,Q,R,diff_weights, gps_measurements, cross_link_times, cross_link_cov, true_bias, true_drift,true_time,dt,N=None):\n",
        "    if N is None: N = [2 for _ in range(len(stations))]\n",
        "\n",
        "    #share first_cross_link\n",
        "    measurements = []\n",
        "\n",
        "    for station in stations:\n",
        "      station.local_time = new_local_times(true_time,true_bias[station.id][0],true_drift[station.id][0])\n",
        "\n",
        "      # time update step\n",
        "      station.time_update(Q,delta_t = dt)\n",
        "\n",
        "      z = gps_measurements[station.id]\n",
        "      measurements.append(z)\n",
        "      station.incremental_update(R,z)\n",
        "    \n",
        "      if cross_link_times:\n",
        "          # initialize psi_n with x from step 2 (can choose from step 1 or 2)\n",
        "          # station.psi_n = np.copy(station.x)\n",
        "          time_ests = {nbhr_id:cross_link_times[nbhr_id] for nbhr_id in station.nbhrs}\n",
        "          cov_ests = {nbhr_id:cross_link_cov[nbhr_id] for nbhr_id in station.nbhrs}\n",
        "          station.crosslink_update(time_ests,cov_ests)\n",
        "      else:\n",
        "          station.psi_n = np.copy(station.x)\n",
        "\n",
        "    for station in stations:\n",
        "      time_ests_Bp = {nbhr_id:stations[nbhr_id].second_cross_link(station.nbhr_dist[nbhr_id]) for nbhr_id in station.nbhrs}\n",
        "      station.diffusion_update(diff_weights, time_ests_Bp)\n",
        "\n",
        "      #Update from T_g,n to T_g,A by adding time between\n",
        "      station.global_est += 3\n",
        "\n",
        "    time_ests_n = [station.global_est for station in stations]\n",
        "    cov_ests_n = [station.P for station in stations]\n",
        "\n",
        "    return measurements,time_ests_n, cov_ests_n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
